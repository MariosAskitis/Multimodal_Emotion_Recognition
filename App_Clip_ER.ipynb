{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo Emotion Recognition: audio + video\n",
    "### Libraries and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You do not have pycocotools installed, so KerasCV pycoco metrics are not available. Please run `pip install pycocotools`.\n",
      "You do not have pyococotools installed, so the `PyCOCOCallback` API is not available.\n",
      "You do not have Waymo Open Dataset installed, so KerasCV Waymo metrics are not available.\n"
     ]
    }
   ],
   "source": [
    "# Utilities\n",
    "import os\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras_cv.layers import RandomCutout\n",
    "# Audio and video manipulation\n",
    "import moviepy.editor as mp\n",
    "import cv2\n",
    "import librosa\n",
    "from joblib import load\n",
    "import tkinter as tk\n",
    "from PIL import ImageTk, Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels dictionary\n",
    "emotions_tras = {1:1, 2:4, 3:5, 4:0, 5:3, 6:2, 7:6}\n",
    "emotions = {0:'angry', 1:'calm', 2:'disgust', 3:'fear', 4:'happy', 5:'sad', 6:'surprise'}\n",
    "\n",
    "# Paths\n",
    "dataset_path = \"./Examples/\"\n",
    "haar_path = './Other/haarcascade_frontalface_default.xml'\n",
    "parameters_path = './Other/audio_test/std_scaler.bin'\n",
    "models_video_path = \"./Models/Video_Stream/\"\n",
    "models_audio_path = \"./Models/Audio_Stream/\"\n",
    "vlc_path = \"D:/Program Files/VLC/vlc.exe\" # to play the selected video (insert your own path to vlc.exe)\n",
    "\n",
    "# Audio video parameters\n",
    "height_targ = 112\n",
    "width_targ = 112\n",
    "sr = 48000"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creatw Window App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "root= tk.Tk()\n",
    "\n",
    "root.title(\"Selection Emotions\")\n",
    "\n",
    "canvas1 = tk.Canvas(root, bg=\"#EEDFCC\", width=650, height=500)\n",
    "canvas1.pack()\n",
    "\n",
    "label1 = tk.Label(root, text='App For Multimodal Emotion Recogntion', bg=\"#EEDFCC\")\n",
    "label1.config(font=('helvetica', 23, 'bold', 'italic'))\n",
    "canvas1.create_window(325, 40, window=label1)\n",
    "\n",
    "label2 = tk.Label(root, text='Demonstration of fusion model (video + audio) for each of 7 emotions.', bg=\"#EEDFCC\")\n",
    "label2.config(font=('Times', 13))\n",
    "canvas1.create_window(325, 100, window=label2)\n",
    "\n",
    "label3 = tk.Label(root, text='Number from 0 to 6:', bg=\"#EEDFCC\")\n",
    "label3.config(font=('helvetica', 0))\n",
    "canvas1.create_window(325, 225, window=label3)\n",
    "\n",
    "label4 = tk.Label(root, text='0. Calm\\n1. Happy\\n2. Sad\\n3. Angry\\n4. Fear\\n5. Disgust\\n6 Surprise', bg=\"#EEDFCC\")\n",
    "label4.config(font=('Times', 13))\n",
    "canvas1.create_window(325, 310, window=label4)\n",
    "\n",
    "def display_text():\n",
    "   global example\n",
    "   example = int(example.get())\n",
    "   root.destroy\n",
    "\n",
    "example = tk.Entry(root)\n",
    "example.pack()\n",
    "canvas1.create_window(325, 150, window=example)\n",
    "\n",
    "    \n",
    "button1 = tk.Button(text='Select Emotion', command=lambda: [display_text(), root.destroy()], font=('helvetica', 12, 'bold'), relief=\"groove\", bg=\"#CDAA7D\")\n",
    "canvas1.create_window(325, 185, window=button1)\n",
    "\n",
    "img = ImageTk.PhotoImage(Image.open(\"./Other/UTH-logo-english.png\").resize((120,120)))\n",
    "label5 = tk.Label(root, image = img, bg=\"#EEDFCC\")\n",
    "canvas1.create_window(100, 430, window=label5)\n",
    "\n",
    "label6 = tk.Label(root, text=\"Marios-Chrysostomos Askitis: 2760\\nFreris Leonardos: 2696\", bg=\"#EEDFCC\")\n",
    "label6.config(font=('Times', 11))\n",
    "canvas1.create_window(325, 440, window=label6)\n",
    "\n",
    "label7 = tk.Label(root, text=\"Autumn 2023\", bg=\"#EEDFCC\")\n",
    "label7.config(font=('Times', 10))\n",
    "canvas1.create_window(590, 470, window=label7)\n",
    "\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transposition of the emotion in order to obtain the ground label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = os.listdir(dataset_path)\n",
    "filename = dataset_path + fn[example]\n",
    "label = emotions_tras[int(fn[example].split('-')[2]) - 1] # trasposition of the emotions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play the selected emotion video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "player = subprocess.call([vlc_path, filename, '--play-and-exit'])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation\n",
    "#### Video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract frames of the video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape frames: (32, 112, 112)\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(filename)\n",
    "haar_cascade = cv2.CascadeClassifier(haar_path)\n",
    "frames = []\n",
    "count = 0\n",
    "skip = 3\n",
    "\n",
    "# Loop through all frames\n",
    "while True:\n",
    "    # Capture frame\n",
    "    ret, frame = cap.read()\n",
    "    if (count % skip == 0 and count > 20):\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        # detect and crop face\n",
    "        faces = haar_cascade.detectMultiScale(frame, scaleFactor=1.12, minNeighbors=9)\n",
    "        if len(faces) != 1:\n",
    "            continue\n",
    "        for (x, y, w, h) in faces:\n",
    "            face = frame[y:y + h, x:x + w]\n",
    "\n",
    "        face = cv2.resize(face, (height_targ+10, width_targ+10))\n",
    "        face = face[5:-5, 5:-5]\n",
    "        face = face/255.\n",
    "        frames.append(face)\n",
    "    count += 1\n",
    "\n",
    "frames = np.array(frames)\n",
    "num_frames = len(frames)\n",
    "labels = [label] * num_frames\n",
    "print('shape frames:', frames.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take the audio from the clip and we save it as .wav file at the folder \"Other\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in ./Other/example.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "audiofile = mp.AudioFileClip(filename).set_fps(sr)\n",
    "audiofile.write_audiofile(\"./Other/example.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the audiofile and we trim it. Then we pad it with zeros in order to has the same length that the model requires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, sampling_rate = librosa.load(\"./Other/example.wav\")\n",
    "y_trimmed, _ = librosa.effects.trim(y, top_db = 30)\n",
    "if(len(y_trimmed) != 93696):\n",
    "    start_pad = (93696 - len(y_trimmed))//2\n",
    "    end_pad = 93696 - len(y_trimmed) - start_pad\n",
    "    y_final = np.pad(y_trimmed, (start_pad, end_pad), mode = 'constant')\n",
    "else:\n",
    "    y_final = y_trimmed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extract the mel spectograms features, we load the scaler parameters and trasform to the required shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 128, 184, 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mel = librosa.power_to_db(librosa.feature.melspectrogram(y = y_final, sr = 48000, n_fft = 1024, n_mels = 128, fmin = 50, fmax = 24000)) \n",
    "\n",
    "scaler = load(parameters_path)\n",
    "mel = scaler.transform(mel)\n",
    "\n",
    "mel = np.expand_dims(mel, axis = 2)\n",
    "mel = np.expand_dims(mel, axis = 0)\n",
    "mel.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load models\n",
    "#### Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_list = os.listdir(models_video_path)\n",
    "model_video = keras.models.load_model(models_video_path + models_list[0])\n",
    "# model_video.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_list = os.listdir(models_audio_path)\n",
    "model_audio = keras.models.load_model(models_audio_path + models_list[0])\n",
    "# model_audio.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions\n",
    "#### Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 286ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1.6348714e-04, 5.4474822e-05, 6.1063957e-03, 9.1054225e-01,\n",
       "       1.1759001e-02, 1.0859572e-02, 6.0514912e-02], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = model_video.predict(frames)\n",
    "pred_video = np.mean(pred, axis=0)\n",
    "pred_video"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 146ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([3.0805247e-06, 5.4812928e-05, 3.0281662e-03, 9.9640155e-01,\n",
       "       1.4702437e-06, 5.1068712e-04, 2.3837696e-07], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = model_audio.predict(mel)\n",
    "pred_audio = np.mean(pred, axis=0)\n",
    "pred_audio"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_global = pred_video + pred_audio # mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video prediction:\t fear\n",
      "Audio prediction:\t fear\n",
      "Global prediction:\t fear\n",
      "Ground truth:\t\t fear\n"
     ]
    }
   ],
   "source": [
    "print('Video prediction:\\t', emotions[pred_video.argmax()])\n",
    "print('Audio prediction:\\t', emotions[pred_audio.argmax()])\n",
    "print('Global prediction:\\t', emotions[pred_global.argmax()])\n",
    "\n",
    "print('Ground truth:\\t\\t', emotions[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Results\n",
    "root = tk.Tk()\n",
    "\n",
    "# root window title and dimension\n",
    "root.title(\"Results\")\n",
    "\n",
    "canvas1 = tk.Canvas(root, bg=\"#EEDFCC\", width=650, height=500)\n",
    "canvas1.pack()\n",
    "\n",
    "label1 = tk.Label(root, text=f'Video prediction:\\t{emotions[pred_video.argmax()]}')\n",
    "label2 = tk.Label(root, text=f'Audio prediction:\\t{emotions[pred_audio.argmax()]}')\n",
    "label3 = tk.Label(root, text=f'Fusion prediction:\\t{emotions[pred_global.argmax()]}')\n",
    "label4 = tk.Label(root, text=f'Actual Label:\\t{emotions[label]}')\n",
    "\n",
    "label1.config(font=('Times', 18), bg=\"#EEDFCC\")\n",
    "label2.config(font=('Times', 18), bg=\"#EEDFCC\")\n",
    "label3.config(font=('Times', 18), bg=\"#EEDFCC\")\n",
    "label4.config(font=('Times', 18, 'bold'), bg=\"#EEDFCC\")\n",
    "\n",
    "canvas1.create_window(325, 75, window=label1)\n",
    "canvas1.create_window(325, 105, window=label2)\n",
    "canvas1.create_window(325, 145, window=label3)\n",
    "canvas1.create_window(325, 210, window=label4)\n",
    "\n",
    "button1 = tk.Button(text='Close', command=lambda: root.destroy(), font=('helvetica', 12, 'bold'), relief=\"groove\", bg=\"#CDAA7D\")\n",
    "canvas1.create_window(325, 300, window=button1)\n",
    "\n",
    "img = ImageTk.PhotoImage(Image.open(\"./Other/UTH-logo-english.png\").resize((120,120)))\n",
    "label5 = tk.Label(root, image = img, bg=\"#EEDFCC\")\n",
    "canvas1.create_window(100, 430, window=label5)\n",
    "\n",
    "label6 = tk.Label(root, text=\"Marios-Chrysostomos Askitis: 2760\\nFreris Leonardos: 2696\", bg=\"#EEDFCC\")\n",
    "label6.config(font=('Times', 11))\n",
    "canvas1.create_window(325, 440, window=label6)\n",
    "\n",
    "label7 = tk.Label(root, text=\"Autumn 2023\", bg=\"#EEDFCC\")\n",
    "label7.config(font=('Times', 10))\n",
    "canvas1.create_window(590, 470, window=label7)\n",
    "\n",
    "root.mainloop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
